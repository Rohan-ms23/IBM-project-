from transformers import BertTokenizer, BertForSequenceClassification, AdamW

# Load BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# Tokenize data
inputs = tokenizer(
    df['cleaned_text'].tolist(),
    padding=True,
    truncation=True,
    max_length=128,
    return_tensors='pt'
)

# Train model
optimizer = AdamW(model.parameters(), lr=2e-5)
for epoch in range(3):
    model.train()
    for batch in DataLoader(TensorDataset(inputs['input_ids'], inputs['attention_mask'], torch.tensor(df['label'])), batch_size=16):
        input_ids, attention_mask, labels = [b.to(device) for b in batch]
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

# Save model
model.save_pretrained("saved_models/bert_model/")

# Attention Visualization
from bertviz import head_view
text = "Wow, you're a genius for spilling coffee on the laptop."
inputs = tokenizer(text, return_tensors='pt')
outputs = model(**inputs, output_attentions=True)
head_view(outputs.attentions, inputs.input_ids, tokenizer)ï¿¼Enter
